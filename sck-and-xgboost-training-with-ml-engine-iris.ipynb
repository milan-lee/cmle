{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Training on Google Cloud Machine Learning Engine\n",
    "This notebook uses the Iris data to demonstrate how to train a model on Cloud Machine Learning Engine (ML Engine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to bring your model to ML Engine\n",
    "Getting your model ready for training can be done in 3 steps:\n",
    "1. Create your python model file\n",
    "    1. Add code to download your data from [Google Cloud Storage](https://cloud.google.com/storage) so that ML Engine can use it\n",
    "    1. Add code to export and save the model to [Google Cloud Storage](https://cloud.google.com/storage) once ML Engine finishes training the model\n",
    "1. Prepare a package\n",
    "1. Submit the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "Before you jump in, let’s cover some of the different tools you’ll be using to get online prediction up and running on ML Engine. \n",
    "\n",
    "[Google Cloud Platform](https://cloud.google.com/) lets you build and host applications and websites, store data, and analyze data on Google's scalable infrastructure.\n",
    "\n",
    "[Cloud ML Engine](https://cloud.google.com/ml-engine/) is a managed service that enables you to easily build machine learning models that work on any type of data, of any size.\n",
    "\n",
    "[Google Cloud Storage](https://cloud.google.com/storage/) (GCS) is a unified object storage for developers and enterprises, from live data serving to data analytics/ML to data archiving.\n",
    "\n",
    "[Cloud SDK](https://cloud.google.com/sdk/) is a command line tool which allows you to interact with Google Cloud products. In order to run this notebook, make sure that Cloud SDK is [installed](https://cloud.google.com/sdk/downloads) in the same environment as your Jupyter kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Setup\n",
    "* [Create a project on GCP](https://cloud.google.com/resource-manager/docs/creating-managing-projects)\n",
    "* [Create a Google Cloud Storage Bucket](https://cloud.google.com/storage/docs/quickstart-console)\n",
    "* [Enable Cloud Machine Learning Engine and Compute Engine APIs](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component&_ga=2.217405014.1312742076.1516128282-1417583630.1516128282)\n",
    "* [Install Cloud SDK](https://cloud.google.com/sdk/downloads)\n",
    "* [[Optional] Install XGBoost](http://xgboost.readthedocs.io/en/latest/build.html)\n",
    "* [[Optional] Install scikit-learn](http://scikit-learn.org/stable/install.html)\n",
    "* [[Optional] Install pandas](https://pandas.pydata.org/pandas-docs/stable/install.html)\n",
    "* [[Optional] Install Google API Python Client](https://github.com/google/google-api-python-client)\n",
    "\n",
    "These variables will be needed for the following steps.\n",
    "* `TRAINER_PACKAGE_PATH <./census_training>` - A packaged training application that will be staged in a Google Cloud Storage location. The model file created below is placed inside this package path.\n",
    "* `MAIN_TRAINER_MODULE <census_training.train>` - Tells ML Engine which file to execute. This is formatted as follows <folder_name.python_file_name>\n",
    "* `JOB_DIR <gs://$BUCKET_ID/xgb_job_dir>` - The path to a Google Cloud Storage location to use for job output.\n",
    "* `RUNTIME_VERSION <1.9>` - The version of Cloud ML Engine to use for the job. If you don't specify a runtime version, the training service uses the default Cloud ML Engine runtime version 1.0. See the list of runtime versions for more information.\n",
    "* `PYTHON_VERSION <3.5>` - The Python version to use for the job. Python 3.5 is available with runtime version 1.4 or greater. If you don't specify a Python version, the training service uses Python 2.7.\n",
    "\n",
    "** Replace: **\n",
    "* `PROJECT_ID <YOUR_PROJECT_ID>` - with your project's id. Use the PROJECT_ID that matches your Google Cloud Platform project.\n",
    "* `BUCKET_ID <YOUR_BUCKET_ID>` - with the bucket id you created above.\n",
    "* `JOB_DIR <gs://YOUR_BUCKET_ID/xgb_job_dir>` - with the bucket id you created above.\n",
    "* `REGION <REGION>` - select a region from [here](https://cloud.google.com/ml-engine/docs/regions) or use the default '`us-central1`'. The region is where the model will be deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_BUCKET = 'gs://test-project-001-210814-acxiom' #CHANGE THIS TO YOUR BUCKET\n",
    "BUCKET_ID = 'test-project-001-210814-acxiom'\n",
    "PROJECT = 'test-project-001-210814' #CHANGE THIS TO YOUR PROJECT ID\n",
    "REGION = 'us-central1' #OPTIONALLY CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GCS_BUCKET'] = GCS_BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Create your python model file\n",
    "\n",
    "First, we'll create the python model file (provided below) that we'll upload to ML Engine. This is similar to your normal process for creating a XGBoost model. However, there are two key differences:\n",
    "1. Downloading the data from GCS at the start of your file, so that ML Engine can access the data.\n",
    "1. Exporting/saving the model to GCS at the end of your file, so that you can use it for predictions.\n",
    "\n",
    "The code in this file loads the data into a pandas DataFrame and pre-processes the data with scikit-learn. This data is then loaded into a DMatrix and used to train a model. Lastly, the model is saved to a file that can be uploaded to [ML Engine's prediction service](https://cloud.google.com/ml-engine/docs/scikit/getting-predictions#deploy_models_and_versions).\n",
    "\n",
    "Note: In normal practice you would want to test your model locally on a small dataset to ensure that it works, before using it with your larger dataset on ML Engine. This avoids wasted time and costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Create Trainer Package\n",
    "Before you can run your trainer application with ML Engine, your code and any dependencies must be placed in a Google Cloud Storage location that your Google Cloud Platform project can access. You can find more info [here](https://cloud.google.com/ml-engine/docs/tensorflow/packaging-trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir trainer\n",
    "touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trainer/task.py\n",
    "#USE THIS FOR SKLEARN EXAMPLE\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "# Fill in your Cloud Storage bucket name\n",
    "BUCKET_ID = 'test-project-001-210814-acxiom'\n",
    "# [END setup]\n",
    "\n",
    "# [START download-data]\n",
    "iris_data_filename = 'iris_data.csv'\n",
    "iris_target_filename = 'iris_target.csv'\n",
    "data_dir = 'gs://cloud-samples-data/ml-engine/iris'\n",
    "\n",
    "# gsutil outputs everything to stderr so we need to divert it to stdout.\n",
    "subprocess.check_call(['gsutil', 'cp', os.path.join(data_dir,\n",
    "                                                    iris_data_filename),\n",
    "                       iris_data_filename], stderr=sys.stdout)\n",
    "subprocess.check_call(['gsutil', 'cp', os.path.join(data_dir,\n",
    "                                                    iris_target_filename),\n",
    "                       iris_target_filename], stderr=sys.stdout)\n",
    "# [END download-data]\n",
    "\n",
    "\n",
    "# [START load-into-pandas]\n",
    "# Load data into pandas\n",
    "iris_data = pd.read_csv(iris_data_filename).values\n",
    "iris_target = pd.read_csv(iris_target_filename).values\n",
    "iris_target = iris_target.reshape((iris_target.size,))\n",
    "# [END load-into-pandas]\n",
    "\n",
    "# Train the model\n",
    "classifier = svm.SVC(verbose=True)\n",
    "classifier.fit(iris_data, iris_target)\n",
    "\n",
    "# Export the classifier to a file\n",
    "model = 'model.joblib'\n",
    "joblib.dump(classifier, model)\n",
    "\n",
    "# [START upload-model]\n",
    "# Upload the saved model file to Cloud Storage\n",
    "model_path = os.path.join('gs://', BUCKET_ID, datetime.datetime.now().strftime(\n",
    "    'iris_%Y%m%d_%H%M%S'), model)\n",
    "subprocess.check_call(['gsutil', 'cp', model, model_path], stderr=sys.stdout)\n",
    "# [END upload-model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trainer/task.py\n",
    "#USE THIS FOR XGBOOST EXAMPLE\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "# Fill in your Cloud Storage bucket name\n",
    "BUCKET_ID = 'test-project-001-210814-acxiom'\n",
    "# [END setup]\n",
    "\n",
    "# [START download-data]\n",
    "iris_data_filename = 'iris_data.csv'\n",
    "iris_target_filename = 'iris_target.csv'\n",
    "data_dir = 'gs://cloud-samples-data/ml-engine/iris'\n",
    "\n",
    "# gsutil outputs everything to stderr so we need to divert it to stdout.\n",
    "subprocess.check_call(['gsutil', 'cp', os.path.join(data_dir,\n",
    "                                                    iris_data_filename),\n",
    "                       iris_data_filename], stderr=sys.stdout)\n",
    "subprocess.check_call(['gsutil', 'cp', os.path.join(data_dir,\n",
    "                                                    iris_target_filename),\n",
    "                       iris_target_filename], stderr=sys.stdout)\n",
    "# [END download-data]\n",
    "\n",
    "\n",
    "# [START load-into-pandas]\n",
    "# Load data into pandas\n",
    "iris_data = pd.read_csv(iris_data_filename).values\n",
    "iris_target = pd.read_csv(iris_target_filename).values\n",
    "iris_target = iris_target.reshape((iris_target.size,))\n",
    "# [END load-into-pandas]\n",
    "\n",
    "# Load data into DMatrix object\n",
    "dtrain = xgb.DMatrix(iris_data, label=iris_target)\n",
    "\n",
    "# Train XGBoost model\n",
    "bst = xgb.train({}, dtrain, 20)\n",
    "\n",
    "# Export the classifier to a file\n",
    "model = 'model.bst'\n",
    "bst.save_model(model)\n",
    "\n",
    "# [START upload-model]\n",
    "# Upload the saved model file to Cloud Storage\n",
    "model_path = os.path.join('gs://', BUCKET_ID, datetime.datetime.now().strftime(\n",
    "    'iris_%Y%m%d_%H%M%S'), model)\n",
    "subprocess.check_call(['gsutil', 'cp', model, model_path], stderr=sys.stdout)\n",
    "# [END upload-model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Submit Training Job\n",
    "Next we need to submit the job for training on ML Engine. We'll use gcloud to submit the job which has the following flags:\n",
    "\n",
    "* `job-name` - A name to use for the job (mixed-case letters, numbers, and underscores only, starting with a letter). In this case: `census_training_$(date +\"%Y%m%d_%H%M%S\")`\n",
    "* `job-dir` - The path to a Google Cloud Storage location to use for job output.\n",
    "* `package-path` - A packaged training application that is staged in a Google Cloud Storage location. If you are using the gcloud command-line tool, this step is largely automated.\n",
    "* `module-name` - The name of the main module in your trainer package. The main module is the Python file you call to start the application. If you use the gcloud command to submit your job, specify the main module name in the --module-name argument. Refer to Python Packages to figure out the module name.\n",
    "* `region` - The Google Cloud Compute region where you want your job to run. You should run your training job in the same region as the Cloud Storage bucket that stores your training data. Select a region from [here](https://cloud.google.com/ml-engine/docs/regions) or use the default '`us-central1`'.\n",
    "* `runtime-version` - The version of Cloud ML Engine to use for the job. If you don't specify a runtime version, the training service uses the default Cloud ML Engine runtime version 1.0. See the list of runtime versions for more information.\n",
    "* `python-version` - The Python version to use for the job. Python 3.5 is available with runtime version 1.4 or greater. If you don't specify a Python version, the training service uses Python 2.7.\n",
    "* `scale-tier` - A scale tier specifying the type of processing cluster to run your job on. This can be the CUSTOM scale tier, in which case you also explicitly specify the number and type of machines to use.\n",
    "\n",
    "Note: Check to make sure gcloud is set to the current PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=trainer \\\n",
    "   -- \\\n",
    "   --output_dir='./output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "JOBNAME=iris_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./trainer \\\n",
    "   --job-dir=$GCS_BUCKET/$JOBNAME/ \\\n",
    "   --runtime-version 1.4 \\\n",
    "   -- \\\n",
    "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional] StackDriver Logging\n",
    "You can view the logs for your training job:\n",
    "1. Go to https://console.cloud.google.com/\n",
    "1. Select \"Logging\" in left-hand pane\n",
    "1. Select \"Cloud ML Job\" resource from the drop-down\n",
    "1. In filter by prefix, use the value of $JOB_NAME to view the logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional] Verify Model File in GCS\n",
    "View the contents of the destination model folder to verify that model file has indeed been uploaded to GCS.\n",
    "\n",
    "Note: The model can take a few minutes to train and show up in GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://$BUCKET_ID/iris_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps:\n",
    "The Cloud Machine Learning Engine online prediction service manages computing resources in the cloud to run your models. Check out the [documentation pages](https://cloud.google.com/ml-engine/docs/scikit/) that describe the process to get online predictions from these exported models using Cloud Machine Learning Engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
